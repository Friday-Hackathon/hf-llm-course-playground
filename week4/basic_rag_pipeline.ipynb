{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acd777c",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§ª Starter RAG Pipeline (Basic)\n",
    "\n",
    "This notebook builds a **minimal Retrieval-Augmented Generation (RAG)** demo using:\n",
    "- **SentenceTransformer** for embeddings (`all-MiniLM-L6-v2`)\n",
    "- **FAISS** for vector search\n",
    "- **FLAN-T5** for lightweight text generation (works on CPU)\n",
    "- Simple file-based dataset (plain `.txt` files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db09822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running in Colab, uncomment:\n",
    "# !pip install -q sentence-transformers faiss-cpu transformers accelerate datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282307eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# === Configuration ===\n",
    "DOCS_DIR = Path(\"./sample_docs\")  # Put your .txt files here\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "GEN_MODEL_NAME = \"google/flan-t5-base\"   # lightweight text2text model (good for Q&A-style prompts)\n",
    "INDEX_DIR = Path(\"./rag_index\")          # where to persist FAISS index + metadata\n",
    "\n",
    "INDEX_DIR.mkdir(exist_ok=True, parents=True)\n",
    "DOCS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Docs folder: {DOCS_DIR.resolve()}\")\n",
    "print(f\"Index folder: {INDEX_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4121c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a few sample docs if folder is empty\n",
    "sample_files = list(DOCS_DIR.glob(\"*.txt\"))\n",
    "if not sample_files:\n",
    "    samples = {\n",
    "        \"faq_bank.txt\": \"\"\"\n",
    "Q: How do I report a lost credit card?\n",
    "A: Call our 24/7 hotline immediately and freeze the card in the mobile app.\n",
    "\n",
    "Q: What is a chargeback?\n",
    "A: A chargeback is a reversal of funds to dispute a card transaction.\n",
    "\"\"\",\n",
    "\n",
    "        \"llm_primer.txt\": \"\"\"\n",
    "Transformers rely on self-attention to model relationships across tokens.\n",
    "Retrieval-Augmented Generation (RAG) pairs an LLM with an external knowledge base.\n",
    "The FLAN-T5 family of models is trained to follow instructions for text generation.\n",
    "\"\"\"\n",
    "    }\n",
    "    for name, content in samples.items():\n",
    "        (DOCS_DIR / name).write_text(content.strip())\n",
    "    print(f\"Created {len(samples)} sample docs in {DOCS_DIR}\")\n",
    "else:\n",
    "    print(f\"Found {len(sample_files)} existing docs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict, Any, List\n",
    "import re\n",
    "\n",
    "def load_text_docs(folder: Path) -> Dict[str, str]:\n",
    "    docs = {}\n",
    "    for p in folder.glob(\"*.txt\"):\n",
    "        docs[p.name] = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return docs\n",
    "\n",
    "def simple_sent_split(text: str) -> List[str]:\n",
    "    parts = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    return [s.strip() for s in parts if s.strip()]\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = 120) -> List[str]:\n",
    "    words = text.split()\n",
    "    chunks, cur = [], []\n",
    "    for w in words:\n",
    "        cur.append(w)\n",
    "        if len(cur) >= max_tokens:\n",
    "            chunks.append(\" \".join(cur))\n",
    "            cur = []\n",
    "    if cur:\n",
    "        chunks.append(\" \".join(cur))\n",
    "    return chunks\n",
    "\n",
    "def build_chunks(docs: Dict[str, str], max_tokens: int = 120) -> List[Dict[str, Any]]:\n",
    "    chunks = []\n",
    "    for source, text in docs.items():\n",
    "        for i, ch in enumerate(chunk_text(text, max_tokens=max_tokens)):\n",
    "            chunks.append({\"text\": ch, \"source\": source, \"chunk_id\": f\"{source}::chunk::{i}\"})\n",
    "    return chunks\n",
    "\n",
    "docs = load_text_docs(DOCS_DIR)\n",
    "chunks = build_chunks(docs, max_tokens=120)\n",
    "print(f\"Loaded {len(docs)} docs -> {len(chunks)} text chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0246090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss, json\n",
    "\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "embeddings = embed_model.encode([c[\"text\"] for c in chunks], show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "faiss.write_index(index, str(INDEX_DIR / \"faiss.index\"))\n",
    "with open(INDEX_DIR / \"meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"chunks\": chunks, \"embed_model\": EMBED_MODEL_NAME}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Index built and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9683ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve(query: str, k: int = 3) -> List[Dict]:\n",
    "    q_emb = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    scores, idxs = index.search(q_emb, k)\n",
    "    results = []\n",
    "    for score, ix in zip(scores[0], idxs[0]):\n",
    "        ch = chunks[ix]\n",
    "        results.append({**ch, \"score\": float(score)})\n",
    "    return results\n",
    "\n",
    "# quick test\n",
    "retrieve(\"How do I report a lost credit card?\", k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999e86e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL_NAME)\n",
    "gen_model = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL_NAME)\n",
    "generator = pipeline(\"text2text-generation\", model=gen_model, tokenizer=gen_tokenizer)\n",
    "\n",
    "def make_prompt(query: str, contexts: List[Dict], max_ctx_chars: int = 1200) -> str:\n",
    "    context_blob = \"\\n\\n\".join([c['text'] for c in contexts])\n",
    "    context_blob = context_blob[:max_ctx_chars]\n",
    "    return f\"\"\"You are a helpful assistant. Use the CONTEXT to answer the QUESTION.\n",
    "\n",
    "CONTEXT:\n",
    "{context_blob}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abb4f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rag_answer(query: str, k: int = 3) -> Dict:\n",
    "    ctx = retrieve(query, k=k)\n",
    "    prompt = make_prompt(query, ctx)\n",
    "    out = generator(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
    "    return {\"query\": query, \"answer\": out, \"contexts\": ctx}\n",
    "\n",
    "rag_answer(\"What is a chargeback?\", k=2)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
